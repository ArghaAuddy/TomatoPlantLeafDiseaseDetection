{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9865c1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 10 classes.\n",
      "Found 500 images belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arghaauddy/Desktop/Tomato Plant Leaf Disease Detection /venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1814s\u001b[0m 6s/step - accuracy: 0.2641 - loss: 2.1367 - val_accuracy: 0.5260 - val_loss: 1.5346\n",
      "Epoch 2/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1129s\u001b[0m 4s/step - accuracy: 0.5480 - loss: 1.4682 - val_accuracy: 0.6560 - val_loss: 1.2259\n",
      "Epoch 3/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1304s\u001b[0m 4s/step - accuracy: 0.6333 - loss: 1.2169 - val_accuracy: 0.6760 - val_loss: 1.0689\n",
      "Epoch 4/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2370s\u001b[0m 8s/step - accuracy: 0.6761 - loss: 1.0782 - val_accuracy: 0.7020 - val_loss: 0.9634\n",
      "Epoch 5/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1274s\u001b[0m 4s/step - accuracy: 0.6961 - loss: 1.0016 - val_accuracy: 0.7340 - val_loss: 0.8954\n",
      "Epoch 6/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2528s\u001b[0m 8s/step - accuracy: 0.7253 - loss: 0.9237 - val_accuracy: 0.7440 - val_loss: 0.8476\n",
      "Epoch 7/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1237s\u001b[0m 4s/step - accuracy: 0.7341 - loss: 0.8756 - val_accuracy: 0.7340 - val_loss: 0.8227\n",
      "Epoch 8/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1269s\u001b[0m 4s/step - accuracy: 0.7452 - loss: 0.8146 - val_accuracy: 0.7560 - val_loss: 0.7608\n",
      "Epoch 9/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2768s\u001b[0m 9s/step - accuracy: 0.7595 - loss: 0.7840 - val_accuracy: 0.7700 - val_loss: 0.7388\n",
      "Epoch 10/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1689s\u001b[0m 5s/step - accuracy: 0.7673 - loss: 0.7520 - val_accuracy: 0.7720 - val_loss: 0.7233\n",
      "Epoch 11/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1199s\u001b[0m 4s/step - accuracy: 0.7781 - loss: 0.7248 - val_accuracy: 0.7880 - val_loss: 0.7030\n",
      "Epoch 12/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1237s\u001b[0m 4s/step - accuracy: 0.7805 - loss: 0.7137 - val_accuracy: 0.7920 - val_loss: 0.6804\n",
      "Epoch 13/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1833s\u001b[0m 6s/step - accuracy: 0.7982 - loss: 0.6720 - val_accuracy: 0.7880 - val_loss: 0.6854\n",
      "Epoch 14/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1195s\u001b[0m 4s/step - accuracy: 0.7910 - loss: 0.6734 - val_accuracy: 0.7940 - val_loss: 0.6500\n",
      "Epoch 15/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1815s\u001b[0m 6s/step - accuracy: 0.7944 - loss: 0.6489 - val_accuracy: 0.7920 - val_loss: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet152V2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Paths\n",
    "train_path = '../data/train'\n",
    "val_path = '../data/new_val'\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_path,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Model Building\n",
    "base_model = ResNet152V2(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze base layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=15\n",
    ")\n",
    "\n",
    "# Save model\n",
    "os.makedirs('../model', exist_ok=True)\n",
    "model.save('../model/tomato_disease_detector.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
